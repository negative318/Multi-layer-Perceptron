# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8c6TwDt6G8zKWN5VYphdg8mvFaXwCxg
"""

import numpy as np
import matplotlib.pyplot as plt
import keras

class activationFunction:
  def sigmoid(Z):
      return np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))

  def relu(Z):
      return np.maximum(0, Z)

  def tanh(Z):
      return np.tanh(Z)

  def softmax(Z):
    Z_max = np.max(Z, axis=0, keepdims=True)
    exp_Z = np.exp(Z - Z_max)
    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)
    return A

class derivative:
    def sigmoid(Z):
      s = activationFunction.sigmoid(Z)
      return s * (1 - s)

    def relu(Z):
      return (Z > 0).astype(float)

    def tanh(Z):
      return 1 - np.tanh(Z) ** 2

class NeuralNetwork:
  def __init__(self, layers_size, activations,loss, l_rate):
    self.layers_size = layers_size
    self.activations = activations
    self.loss = loss
    self.l_rate = l_rate
    self.num_layer = len(layers_size)
    self.W = [0]
    self.b = [0]
    for i in range(1,self.num_layer,1):
      self.W.append(np.random.randn(layers_size[i-1],layers_size[i]))
      self.b.append(np.random.randn(layers_size[i],1))


  def forward(self, inputs):
    self.A = [inputs]
    self.Z = [0]
    for i in range(1,self.num_layer,1):
      z = np.dot(self.W[i].T, self.A[-1]) + self.b[i]
      self.Z.append(z)
      self.A.append(self.activeFuncion(z,self.activations[i]))
    return self.A[-1]

  def backpropagation(self,output):
    E = []
    dW = []
    db = []
    for i in range(self.num_layer - 1, 0, -1):
      if i == self.num_layer - 1:
        E.append(self.derivative(output,self.Z[i], self.activations[i]))
        dW.insert(0, np.dot(self.A[i-1], E[-1].T))
        db.insert(0, np.sum(E[-1], axis=1, keepdims=True))
      else:
        E.append(self.derivative(np.dot(self.W[i+1], E[-1]),self.Z[i],self.activations[i]))
        dW.insert(0, np.dot(self.A[i-1], E[-1].T))
        db.insert(0, np.sum(E[-1], axis=1, keepdims=True))
    dW.insert(0,0)
    db.insert(0,0)
    E.append(np.dot(self.W[1], E[-1]))
    for i in range(self.num_layer):
        self.W[i] -= dW[i] * self.l_rate
        self.b[i] -= db[i] * self.l_rate
    return E[-1]


  def activeFuncion(self,Z,active):
    if active == "sigmoid":
      return activationFunction.sigmoid(Z)
    elif active == "relu":
      return activationFunction.relu(Z)
    elif active == "tanh":
      return activationFunction.tanh(Z)
    elif active == "softmax":
      return activationFunction.softmax(Z)


  def derivative(self,E,Z,active):
    if active == "sigmoid":
      return derivative.sigmoid(E)
    elif active == "relu":
      return E * derivative.relu(Z)
    elif active == "tanh":
      return derivative.tanh(E)
    elif active == "softmax":
      return 1/E.size * (self.A[-1] - E)


  def cost(self,ouput,func):
      if func == 'MAE':
          return np.abs(self.A[-1] - ouput)
      elif func == 'MSE':
          return (self.A[-1] - ouput)**2/2
      elif func == 'crossEntropy':
          return -np.sum(ouput * np.log(self.A[-1]+1e-6))/ouput.shape[1]
      elif func == 'binaryCrossEntropy':
          return -np.sum(ouput * np.log(self.A[-1]+1e-6) + (1 - ouput) * np.log(1 - self.A[-1]+1e-6))


  def get_accuracy(self,predictions,output):
    return np.sum(predictions == output) / output.size


  def train(self,input,output,val_in, val_out, epochs,batch_size =128):
    for i in range(epochs):
      for j in range(0,input.shape[1],batch_size):
        input_batch = input[:,j:j+batch_size]
        output_batch = output[:,j:j+batch_size]
        self.forward(input_batch)
        self.backpropagation(output_batch)
        if i % 100 == 0 and j == 0:
          print(i,"loss: ", self.cost(output_batch,self.loss),
                "Accuracy train: ", self.get_accuracy(np.argmax(self.A[-1],0),np.argmax(output_batch,0)),
                "Accuracy validation: ", self.test(val_in,val_out)
              )


  def test(self,val_in,val_out):
    self.forward(val_in)
    return (self.get_accuracy(np.argmax(self.A[-1],0),np.argmax(val_out,0)))

import numpy as np
# from MLP import *

from keras.datasets import mnist
(x_train,y_train),(x_test,y_test) = mnist.load_data()

def one_hot(Y):
  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))
  one_hot_Y[np.arange(Y.size),Y] = 1
  one_hot_Y = one_hot_Y.T
  return one_hot_Y

num_test = 1000
y_train = one_hot(y_train)
y_test = one_hot(y_test)
x_train = x_train/255
x_test = x_test/255
x_validation = x_train[56000:60000]
y_validation = y_train[:,56000:60000]
x_train = x_train[0:num_test]
y_train = y_train[:,0:num_test]

print(x_train.shape,y_train.shape)
print(x_test.shape,y_test.shape)

class Convolutional:


  # input (Di,Hi,Wi)
  # output (Do,Ho,Wo)
  # kernel (Do,Di,Hk,Wk)
  # bias (Do,Ho,Wo)
  def __init__(self, input_shape, kernel_size, output_depth, l_rate):
    input_depth, input_height, input_width = input_shape
    output_height = (input_height - kernel_size) + 1
    output_width = (input_width - kernel_size) + 1
    self.depth = output_depth


    self.input_shape = input_shape
    self.kernel_shape = (output_depth, input_depth, kernel_size, kernel_size)
    self.bias_shape = (output_depth, output_height, output_width)
    self.output_shape = (output_depth, output_height, output_width)

    self.kernel = np.random.randn(*self.kernel_shape)
    self.bias = np.random.randn(*self.bias_shape)
    # count = 0
    # for i in range(self.kernel_shape[0]):
    #   for j in range(self.kernel_shape[1]):
    #     for k in range(self.kernel_shape[2]):
    #       for l in range(self.kernel_shape[3]):
    #         self.kernel[i][j][k][l] = count;
    #         if(count == 0): count = 1
    #         else: count =0
    # for i in range(self.bias_shape[0]):
    #   for j in range(self.bias_shape[1]):
    #     for k in range(self.bias_shape[2]):
    #       self.bias[i][j][k] = 1;
    # print(self.kernel)
    # print(self.bias)

    self.l_rate = l_rate

  # input (Di,Hi,Wi)
  # img (Hi, Wi)
  # kernel(Hk,Wk)
  def conv(self, image, kernel, status):
    image = np.array(image)
    kernel = np.array(kernel)
    img_shape = image.shape
    kernel_shape = kernel.shape
    if status == "forward":
      out_put_height = img_shape[0]-kernel_shape[0]+1
      out_put_width = img_shape[1]-kernel_shape[1]+1
      output = np.zeros((out_put_height, out_put_width))
      for i in range (out_put_height):
        for j in range (out_put_width):
          output[i][j] = np.sum(image[i:i+kernel_shape[0],j:j + kernel_shape[1]]*kernel)
      return output

    if status == "backprop":

      out_put_height = img_shape[0]+kernel_shape[0]-1
      out_put_width = img_shape[1]+kernel_shape[1]-1
      output = np.zeros((out_put_height, out_put_width))

      # input_height, input_width = image.shape
      # input_height = input_height + (kernel_shape[0]-1)*2
      # input_width = input_width + (kernel_shape[1]-1)*2
      # input = np.zeros((input_height, input_width))
      # for i in range (input_height):
      #   for j in range (input_width):
      #     if i < kernel_shape[0]-1 or j < kernel_shape[1]-1 or i > input_height-kernel_shape[0] or j > input_width-kernel_shape[1]:
      #       input[i][j] = 0
      #     else:
      #       input[i][j] = image[i-kernel_shape[0]+1][j-kernel_shape[1]+1]

      padded_image = np.pad(image, ((kernel_shape[0] - 1, kernel_shape[0] - 1), (kernel_shape[1] - 1, kernel_shape[1] - 1)), mode='constant')

      for i in range (out_put_height):
        for j in range (out_put_width):
          output[i][j] = np.sum(padded_image[i:i+kernel_shape[0],j:j + kernel_shape[1]]*kernel)
      return output

  # input (Di,Hi,Wi)
  # output (Do,Ho,Wo)
  # kernel (Do,Di,Hk,Wk)
  # bias (Do,Ho,Wo)
  def forward(self, input):
    self.input = np.array(input)
    self.output = np.zeros(self.output_shape)
    for j in range (self.output_shape[0]):
      for i in range (self.input_shape[0]):
          self.output[j] += self.conv(self.input[i], self.kernel[j][i],"forward")
      self.output[j] += self.bias[j]
    return self.output


  #gradY (Dy, Hy, Wy)
  # input (Di,Hi,Wi)
  # output (Do,Ho,Wo)
  # kernel (Do,Di,Hk,Wk)
  # bias (Do,Ho,Wo)
  def backpropagation(self, gradY):
    grad_kernel = np.zeros(self.kernel_shape)
    grad_input = np.zeros(self.input_shape)



# xoay 180
    root_kernel = np.zeros(self.kernel_shape)

    for i in range (self.output_shape[0]):
      for j in range(self.input_shape[0]):
        root_kernel[i][j] = np.rot90(self.kernel[i][j], 2)
        grad_input[j] += self.conv(gradY[i], root_kernel[i][j],"backprop")
        grad_kernel[i][j] = self.conv(self.input[j], gradY[i],"forward")
    # print("grad_kernel",grad_kernel)
    # print("grad_bias",gradY)
    # print("grad_input",grad_input)
    self.kernel -= self.l_rate*grad_kernel
    self.bias -= self.l_rate*gradY
    return grad_input

array = np.arange(1, 76).reshape(1,3, 5, 5)
root_kernel = np.zeros(array.shape)
for i in range(array.shape[0]):
  for j in range(array.shape[1]):
     root_kernel[i][j] = np.rot90(array[i][j], 2)
# print(root_kernel)

array = np.arange(1, 26).reshape(1, 5, 5)
# print(array)
CNN = Convolutional((1,5,5),3,3,0.1)
output = CNN.forward(array)
# print(output)
back = CNN.backpropagation(output)
# print(back)

class MaxPoolingLayer:
  def __init__(self, pool_size):
    self.pool_size = pool_size

  def forward(self, input): # input 3 chi·ªÅu
    self.input = np.array(input)
    out_depth = self.input.shape[0]
    out_height = int(self.input.shape[1]/self.pool_size)
    out_width = int(self.input.shape[2]/self.pool_size)
    if(self.input.shape[1] % self.pool_size != 0):
      out_height += 1
    if(self.input.shape[2] % self.pool_size != 0):
      out_width += 1
    self.output = np.zeros((out_depth, out_height, out_width))
    for d in range(out_depth):
      for i in range(0, out_height):
        for j in range(0, out_width):
          start_row = i * self.pool_size
          end_row = min(start_row + self.pool_size, self.input.shape[1])
          start_col = j * self.pool_size
          end_col = min(start_col + self.pool_size, self.input.shape[2])
          if start_row < end_row and start_col < end_col:
              self.output[d][i][j] = np.max(self.input[d,start_row:end_row, start_col:end_col])
    return self.output
  def backpropagation(self, gradY):
    grad_input = np.zeros(self.input.shape)
    for d in range(gradY.shape[0]):
      for i in range(0, gradY.shape[1]):
        for j in range(0, gradY.shape[2]):
          start_row = i * self.pool_size
          end_row = min(start_row + self.pool_size, self.input.shape[1])
          start_col = j * self.pool_size
          end_col = min(start_col + self.pool_size, self.input.shape[2])
          if start_row < end_row and start_col < end_col:
            max_val = np.max(self.input[d,start_row:end_row, start_col:end_col])
            max_indices = np.where(self.input[d,start_row:end_row, start_col:end_col] == max_val)
            max_indices = (max_indices[0] + start_row, max_indices[1] + start_col)
            grad_input[d][max_indices[0][0]][max_indices[1][0]] = gradY[d][i][j]
    return grad_input

class Flattening():
  def __init__(self):
    pass
  def forward(self, input):
    input = np.array(input)
    self.input = input
    self.output = input.reshape(input.shape[0], -1)
    return self.output.T
  def backpropagation(self, gradY):
    return gradY.T.reshape(self.input.shape)

class Model:
  def __init__(self, CNN_layers, nn_layer ,batch_size, epochs, l_rate):
    self.CNN_layers = CNN_layers
    self.nn_layer = nn_layer
    self.batch_size = batch_size
    self.epochs = epochs
    self.l_rate = l_rate
    self.flatten = Flattening()
  def train(self, x_train,y_train,x_val,y_val):
    for e in range(self.epochs):
      for i in range(0,x_train.shape[0],self.batch_size):
        x_batch = x_train[i:min(i+batch_size,x_train.shape[0])]
        y_batch = y_train[:,i:min(i+batch_size,x_train.shape[0])] # 10x32
        output = [0] * x_batch.shape[0]


        # forward prop
        for j in range(x_batch.shape[0]):
          output[j] = np.expand_dims(x_batch[j], axis=0)
          for layer in self.CNN_layers:
            output[j] = layer.forward(output[j])
        output_array = self.flatten.forward(output)
        Y_hat = self.nn_layer.forward(output_array)
        if(i%10 == 0):
          print("epochs: ", e, "interation:", i, "loss: ", nn.cost(y_batch,nn.loss), "accuracy_train:", nn.get_accuracy(np.argmax(Y_hat,0),np.argmax(y_batch,0)))

        # backprop
        gradY = self.nn_layer.backpropagation(y_batch)
        gradY = self.flatten.backpropagation(gradY)
        back_prop  = []
        for j in range(x_batch.shape[0]):
          back_prop.append(gradY[j])
          for layer in reversed(self.CNN_layers):
            back_prop.append(layer.backpropagation(back_prop[-1]))

epochs = 30
batch_size = 32
CNN1 = Convolutional((1,28,28),3,4,0.1)
pool1 = MaxPoolingLayer(2)
CNN2 = Convolutional((4,13,13),3,8,0.1)
pool2 = MaxPoolingLayer(2)
flatten = Flattening()
nn = NeuralNetwork(layers_size=[288,10],activations = ["", "softmax"], loss = "crossEntropy", l_rate = 0.1) # 8*6*6 = 576



model = Model([CNN1,pool1,CNN2,pool2],nn,batch_size,epochs,0.1)
model.train(x_train,y_train,x_validation,y_validation)


# print(x_train.shape[0])
# for e in range(epochs):
#   for i in range(0,x_train.shape[0],batch_size):
#       x_batch = x_train[i:min(i+batch_size,x_train.shape[0])]
#       y_batch = y_train[:,i:min(i+batch_size,x_train.shape[0])] # 10x32
#       output = [0] * x_batch.shape[0]


#       # forward prop
#       for j in range(x_batch.shape[0]):
#         # print("bias:",CNN1.bias)
#         # print("kernel:",CNN1.kernel)
#         x_batch_reshaped = np.expand_dims(x_batch[j], axis=0)
#         output[j] = CNN1.forward(x_batch_reshaped)
#         output[j] = pool1.forward(output[j])
#         output[j] = CNN2.forward(output[j])
#         output[j] = pool2.forward(output[j])
#       output_array = flatten.forward(output)
#       #print("output_array: ", output_array)
#       Y_hat = nn.forward(output_array)
#       if(i%10 == 0):
#         print("epochs: ", e, "interation:", i, "loss: ", nn.cost(y_batch,nn.loss), "accuracy_train:", nn.get_accuracy(np.argmax(Y_hat,0),np.argmax(y_batch,0)), "accuracy_validation:", nn.test(x_validation,y_validation))

#       # backprop
#       gradY = nn.backpropagation(y_batch)
#       #print("gradY: ", gradY)
#       gradY = flatten.backpropagation(gradY)
#       back_prop  = []
#       for j in range(x_batch.shape[0]):
#         back_prop.append(gradY[j])
#         back_prop.append(pool2.backpropagation(back_prop[-1]))
#         back_prop.append(CNN2.backpropagation(back_prop[-1],0.1))
#         back_prop.append(pool1.backpropagation(back_prop[-1]))
#         back_prop.append(CNN1.backpropagation(back_prop[-1],0.1))
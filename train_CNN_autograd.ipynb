{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28) (10, 1000)\n",
      "(1000, 1, 28, 28) (10, 1000)\n",
      "(10000, 1, 28, 28) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "from CNN_autograd import *\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))\n",
    "  one_hot_Y[np.arange(Y.size),Y] = 1\n",
    "  one_hot_Y = one_hot_Y.T\n",
    "  return one_hot_Y\n",
    "\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "x_validation = x_train[59000:60000]\n",
    "y_validation = y_train[:,59000:60000]\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[:,0:1000]\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],1,28,28)\n",
    "x_validation = x_validation.reshape(x_validation.shape[0],1,28,28)\n",
    "x_test = x_test.reshape(x_test.shape[0],1,28,28)\n",
    "\n",
    "# x_train = Tensor(x_train.reshape(x_train.shape[0],1,28,28), requires_grad=True)\n",
    "# x_validation = Tensor(x_validation.reshape(x_validation.shape[0],1,28,28), requires_grad=True)\n",
    "# x_test = Tensor(x_test.reshape(x_test.shape[0],1,28,28), requires_grad=True)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_validation.shape, y_validation.shape)\n",
    "print(x_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "CNN1 = Convolutional(input_shape = (1,28,28), kernel_size = 3, output_depth = 4, l_rate = 0.0001, activeFuncion = \"relu\")\n",
    "pool1 = MaxPoolingLayer(2)\n",
    "CNN2 = Convolutional(input_shape = (4,13,13), kernel_size = 3,output_depth = 8, l_rate = 0.0001, activeFuncion = \"relu\")\n",
    "pool2 = MaxPoolingLayer(2)\n",
    "flatten = Flattening()\n",
    "nn = NeuralNetwork(layers_size=[288,10],activations = [\"softmax\"], lossFunction = \"crossEntropy\", l_rate = 0.0001) # 8*6*6 = 288\n",
    "\n",
    "# a = Dense()\n",
    "\n",
    "# a.add(CNN1)\n",
    "# a.add(pool1)\n",
    "# a.add(CNN2)\n",
    "# a.add(flatten)\n",
    "\n",
    "# a.forward()\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.144400749842347 accuracy: 0.0625\n",
      "loss: 10.689881267874787 accuracy: 0.046875\n",
      "loss: 11.26764555198551 accuracy: 0.09375\n",
      "loss: 10.706205174589229 accuracy: 0.0625\n",
      "loss: 11.724017291548556 accuracy: 0.03125\n",
      "loss: 12.310466769909311 accuracy: 0.015625\n",
      "loss: 11.465019003599267 accuracy: 0.015625\n",
      "loss: 11.625379967725355 accuracy: 0.015625\n",
      "loss: 12.103830250186974 accuracy: 0.078125\n",
      "loss: 11.659275183202606 accuracy: 0.03125\n",
      "loss: 11.29980790179815 accuracy: 0.078125\n",
      "loss: 11.499147624819589 accuracy: 0.046875\n",
      "loss: 11.769100011318542 accuracy: 0.03125\n",
      "loss: 11.613804055401173 accuracy: 0.03125\n",
      "loss: 12.202995114833426 accuracy: 0.03125\n",
      "loss: 10.8647013242559 accuracy: 0.0\n",
      "loss: 11.127662336604779 accuracy: 0.0625\n",
      "loss: 10.720081751492216 accuracy: 0.046875\n",
      "loss: 11.221664888550599 accuracy: 0.09375\n",
      "loss: 10.713995220169103 accuracy: 0.0625\n",
      "loss: 11.717081294987302 accuracy: 0.03125\n",
      "loss: 12.319583326640755 accuracy: 0.015625\n",
      "loss: 11.450072073702074 accuracy: 0.015625\n",
      "loss: 11.61969669758954 accuracy: 0.015625\n",
      "loss: 12.093000085671417 accuracy: 0.078125\n",
      "loss: 11.666447811035344 accuracy: 0.03125\n",
      "loss: 11.292182476086957 accuracy: 0.078125\n",
      "loss: 11.496787373503174 accuracy: 0.03125\n",
      "loss: 11.745999266343055 accuracy: 0.03125\n",
      "loss: 11.601556340212941 accuracy: 0.03125\n",
      "loss: 12.19233214486143 accuracy: 0.03125\n",
      "loss: 10.869105248660428 accuracy: 0.0\n",
      "loss: 11.109533710227648 accuracy: 0.0625\n",
      "loss: 10.755248997732336 accuracy: 0.046875\n",
      "loss: 11.173896883890105 accuracy: 0.09375\n",
      "loss: 10.723846576929754 accuracy: 0.0625\n",
      "loss: 11.710741999533607 accuracy: 0.03125\n",
      "loss: 12.336245654913355 accuracy: 0.015625\n",
      "loss: 11.436159063462515 accuracy: 0.015625\n",
      "loss: 11.610514216996446 accuracy: 0.015625\n",
      "loss: 12.080822581091546 accuracy: 0.078125\n",
      "loss: 11.671115308302522 accuracy: 0.03125\n",
      "loss: 11.285760356462838 accuracy: 0.078125\n",
      "loss: 11.491434498248054 accuracy: 0.03125\n",
      "loss: 11.719808462330608 accuracy: 0.03125\n",
      "loss: 11.590489077420237 accuracy: 0.03125\n",
      "loss: 12.181892826386491 accuracy: 0.03125\n",
      "loss: 10.866978770179552 accuracy: 0.0\n",
      "loss: 11.094799407742382 accuracy: 0.0625\n",
      "loss: 10.794349453787557 accuracy: 0.046875\n",
      "loss: 11.128558141015516 accuracy: 0.09375\n",
      "loss: 10.739578036270014 accuracy: 0.0625\n",
      "loss: 11.707385290043135 accuracy: 0.03125\n",
      "loss: 12.354053297869475 accuracy: 0.015625\n",
      "loss: 11.424493969987395 accuracy: 0.015625\n",
      "loss: 11.60015271276598 accuracy: 0.015625\n",
      "loss: 12.066394512159729 accuracy: 0.078125\n",
      "loss: 11.667231009583956 accuracy: 0.03125\n",
      "loss: 11.28010010108004 accuracy: 0.078125\n",
      "loss: 11.484018746651671 accuracy: 0.03125\n",
      "loss: 11.697787870236258 accuracy: 0.03125\n",
      "loss: 11.585098764109372 accuracy: 0.03125\n",
      "loss: 12.171970342340533 accuracy: 0.03125\n",
      "loss: 10.861398066899032 accuracy: 0.0\n",
      "loss: 11.085636526449688 accuracy: 0.0625\n",
      "loss: 10.83583930482313 accuracy: 0.046875\n",
      "loss: 11.082413247550583 accuracy: 0.09375\n",
      "loss: 10.764375847758032 accuracy: 0.0625\n",
      "loss: 11.7084644517415 accuracy: 0.03125\n",
      "loss: 12.368866774464282 accuracy: 0.015625\n",
      "loss: 11.412946623736447 accuracy: 0.015625\n",
      "loss: 11.586893624640126 accuracy: 0.015625\n",
      "loss: 12.05425266226802 accuracy: 0.0625\n",
      "loss: 11.656045133082653 accuracy: 0.03125\n",
      "loss: 11.273680799966442 accuracy: 0.0625\n",
      "loss: 11.47511650220843 accuracy: 0.03125\n",
      "loss: 11.674077300303313 accuracy: 0.03125\n",
      "loss: 11.588503501244247 accuracy: 0.03125\n",
      "loss: 12.161002453774383 accuracy: 0.03125\n",
      "loss: 10.855620465429066 accuracy: 0.0\n",
      "loss: 11.072828336611664 accuracy: 0.0625\n",
      "loss: 10.878230734767948 accuracy: 0.046875\n",
      "loss: 11.029269905046533 accuracy: 0.09375\n",
      "loss: 10.791241631996229 accuracy: 0.0625\n",
      "loss: 11.710531402143125 accuracy: 0.03125\n",
      "loss: 12.379892500197741 accuracy: 0.015625\n",
      "loss: 11.401325484106021 accuracy: 0.015625\n",
      "loss: 11.571189200617727 accuracy: 0.015625\n",
      "loss: 12.04306447543565 accuracy: 0.0625\n",
      "loss: 11.643867492090472 accuracy: 0.03125\n",
      "loss: 11.269303377866695 accuracy: 0.0625\n",
      "loss: 11.465364656947749 accuracy: 0.03125\n",
      "loss: 11.657442292529877 accuracy: 0.03125\n",
      "loss: 11.601127915073821 accuracy: 0.03125\n",
      "loss: 12.149221873772202 accuracy: 0.03125\n",
      "loss: 10.854984928811671 accuracy: 0.0\n",
      "loss: 11.054257530463103 accuracy: 0.046875\n",
      "loss: 10.920977643592794 accuracy: 0.046875\n",
      "loss: 10.966298654026385 accuracy: 0.09375\n",
      "loss: 10.81572556653135 accuracy: 0.0625\n",
      "loss: 11.714557556807883 accuracy: 0.03125\n",
      "loss: 12.385623019083472 accuracy: 0.015625\n",
      "loss: 11.387199547316499 accuracy: 0.015625\n",
      "loss: 11.551924518726125 accuracy: 0.015625\n",
      "loss: 12.031966474710195 accuracy: 0.0625\n",
      "loss: 11.634842531653742 accuracy: 0.03125\n",
      "loss: 11.267827817306586 accuracy: 0.0625\n",
      "loss: 11.46223824728915 accuracy: 0.03125\n",
      "loss: 11.644108229778682 accuracy: 0.03125\n",
      "loss: 11.617358003801051 accuracy: 0.03125\n",
      "loss: 12.136716346527752 accuracy: 0.03125\n",
      "loss: 10.858614212359607 accuracy: 0.0\n",
      "loss: 11.03251330688846 accuracy: 0.046875\n",
      "loss: 10.959432920687325 accuracy: 0.046875\n",
      "loss: 10.902138797139614 accuracy: 0.09375\n",
      "loss: 10.832901330278627 accuracy: 0.0625\n",
      "loss: 11.717516429629887 accuracy: 0.03125\n",
      "loss: 12.379723368457281 accuracy: 0.015625\n",
      "loss: 11.37121815116877 accuracy: 0.015625\n",
      "loss: 11.52807807680156 accuracy: 0.015625\n",
      "loss: 12.01838573918431 accuracy: 0.0625\n",
      "loss: 11.630833154468784 accuracy: 0.03125\n",
      "loss: 11.269914134688257 accuracy: 0.0625\n",
      "loss: 11.462922306585828 accuracy: 0.03125\n",
      "loss: 11.624599462025195 accuracy: 0.03125\n",
      "loss: 11.634711276508936 accuracy: 0.03125\n",
      "loss: 12.124789230515093 accuracy: 0.03125\n",
      "loss: 10.869432502003425 accuracy: 0.0\n",
      "loss: 11.00188880197284 accuracy: 0.03125\n",
      "loss: 10.99164614907571 accuracy: 0.046875\n",
      "loss: 10.835867106970225 accuracy: 0.09375\n",
      "loss: 10.842921786130216 accuracy: 0.0625\n",
      "loss: 11.717739818243391 accuracy: 0.03125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m flatten\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[1;32m---> 18\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(CNN1.bias)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m nn\u001b[38;5;241m.\u001b[39mupdate_parameter()\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:233\u001b[0m, in \u001b[0;36mTensor.softmax.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    232\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (result\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m grad) \u001b[38;5;241m/\u001b[39m grad\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m--> 233\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:39\u001b[0m, in \u001b[0;36mTensor.__add__.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     37\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Summing along the second axis\u001b[39;00m\n\u001b[0;32m     38\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m new_self_grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_self_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# Reduce the grad shape to match other.data.shape\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:128\u001b[0m, in \u001b[0;36mTensor.dot.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    125\u001b[0m other_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata), grad)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# print(\"bbbbbbbbbbbbbb\", np.sum(other_grad))\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# print(\"bbbbbbbbbbbbbb\", other_grad)\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:22\u001b[0m, in \u001b[0;36mTensor.T.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(grad):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:330\u001b[0m, in \u001b[0;36mTensor.flatten.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    329\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mreshape(input_shape)\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:317\u001b[0m, in \u001b[0;36mTensor.maxpooling.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    314\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[n, c, h, w] \u001b[38;5;241m==\u001b[39m max_val:\n\u001b[0;32m    315\u001b[0m                             grad_input[n, c, h, w] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grad[n, c, i, j]\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:343\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\Máy tính\\Multi-layer-Perceptron\\autograd.py:166\u001b[0m, in \u001b[0;36mTensor.relu.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    164\u001b[0m     result \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m0\u001b[39m), depends_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad, operator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(grad):\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    168\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(grad \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "def get_accuracy(predictions,output):\n",
    "    return np.sum(predictions == output) / output.shape[1]\n",
    "\n",
    "for i in range (epochs):\n",
    "\n",
    "    for i in range(0,x_train.shape[0],64):\n",
    "        x_batch = x_train[i:min(i+batch_size,x_train.shape[0])]\n",
    "        y_batch = y_train[:,i:min(i+batch_size,x_train.shape[0])] # 10x32\n",
    "\n",
    "        output = CNN1.forward(Tensor(x_batch, requires_grad=True))\n",
    "        output = pool1.forward(output)\n",
    "        output = CNN2.forward(output)\n",
    "        output = pool2.forward(output)\n",
    "        output = flatten.forward(output)\n",
    "        output = nn.forward(output)\n",
    "\n",
    "        output.backward(y_batch)\n",
    "        # print(CNN1.bias)\n",
    "\n",
    "        nn.update_parameter()\n",
    "        CNN2.update_parameter()\n",
    "        CNN1.update_parameter()\n",
    "\n",
    "        print(\"loss:\", -np.sum(y_batch * np.log(output.data+1e-6)) / y_batch.shape[1], \"accuracy:\", get_accuracy(output.data,y_batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

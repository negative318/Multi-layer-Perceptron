{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784) (54000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(6000, 784) (6000, 10)\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train /255.0\n",
    "x_test = x_test /255.0\n",
    "x_validation = x_train[:6000]\n",
    "y_validation = y_train[:6000]\n",
    "x_train = x_train[6000:]\n",
    "y_train = y_train[6000:]\n",
    "\n",
    "x_validation = x_validation.reshape(x_validation.shape[0],-1)\n",
    "x_train = x_train.reshape(x_train.shape[0],-1)\n",
    "x_test = x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))\n",
    "  one_hot_Y[np.arange(Y.size),Y] = 1\n",
    "  return one_hot_Y\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_validation = one_hot(y_validation)\n",
    "y_test = one_hot(y_test)\n",
    "\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "print(x_validation.shape,y_validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  10.189996691752748 Accuracy train:  0.078125 Accuracy validation:  0.0685\n",
      "100 loss:  0.8361323297530706 Accuracy train:  0.84375 Accuracy validation:  0.782\n",
      "200 loss:  0.6703487054163441 Accuracy train:  0.8984375 Accuracy validation:  0.8365\n",
      "300 loss:  0.6082156932429406 Accuracy train:  0.90625 Accuracy validation:  0.8556666666666667\n",
      "400 loss:  0.5672659525412593 Accuracy train:  0.9140625 Accuracy validation:  0.8675\n",
      "500 loss:  0.5345823896576796 Accuracy train:  0.921875 Accuracy validation:  0.874\n",
      "600 loss:  0.507143592252206 Accuracy train:  0.921875 Accuracy validation:  0.8773333333333333\n",
      "700 loss:  0.4840036963303526 Accuracy train:  0.921875 Accuracy validation:  0.881\n",
      "800 loss:  0.46447443000970445 Accuracy train:  0.9296875 Accuracy validation:  0.8823333333333333\n",
      "900 loss:  0.4479618238408157 Accuracy train:  0.9296875 Accuracy validation:  0.886\n",
      "1000 loss:  0.4339525344203724 Accuracy train:  0.921875 Accuracy validation:  0.8891666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8898"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[28*28, 10],activations = [\"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train, x_validation, y_validation, epochs=1001, batch_size=128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  12.305059091883892 Accuracy train:  0.109375 Accuracy validation:  0.12\n",
      "100 loss:  0.5083524948870141 Accuracy train:  0.890625 Accuracy validation:  0.8428333333333333\n",
      "200 loss:  0.356098583188289 Accuracy train:  0.9140625 Accuracy validation:  0.8685\n",
      "300 loss:  0.2954939799284341 Accuracy train:  0.9296875 Accuracy validation:  0.882\n",
      "400 loss:  0.2601446431414607 Accuracy train:  0.953125 Accuracy validation:  0.8916666666666667\n",
      "500 loss:  0.23961469252132023 Accuracy train:  0.9609375 Accuracy validation:  0.899\n",
      "600 loss:  0.22550380708960882 Accuracy train:  0.96875 Accuracy validation:  0.9046666666666666\n",
      "700 loss:  0.2164503283089067 Accuracy train:  0.96875 Accuracy validation:  0.9093333333333333\n",
      "800 loss:  0.20859003576781482 Accuracy train:  0.96875 Accuracy validation:  0.9121666666666667\n",
      "900 loss:  0.2016555180547753 Accuracy train:  0.96875 Accuracy validation:  0.9156666666666666\n",
      "1000 loss:  0.19497887347831205 Accuracy train:  0.96875 Accuracy validation:  0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9174"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[28*28,64, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  12.18412451171531 Accuracy train:  0.1015625 Accuracy validation:  0.12566666666666668\n",
      "100 loss:  0.16390661047981253 Accuracy train:  0.9609375 Accuracy validation:  0.9301666666666667\n",
      "200 loss:  0.11912899831350288 Accuracy train:  0.9765625 Accuracy validation:  0.9403333333333334\n",
      "300 loss:  0.0915678192162841 Accuracy train:  0.9765625 Accuracy validation:  0.9455\n",
      "400 loss:  0.07180428561549385 Accuracy train:  0.9921875 Accuracy validation:  0.9483333333333334\n",
      "500 loss:  0.05506154745939609 Accuracy train:  0.9921875 Accuracy validation:  0.9505\n",
      "600 loss:  0.04108949069652539 Accuracy train:  0.9921875 Accuracy validation:  0.9526666666666667\n",
      "700 loss:  0.029534096726067476 Accuracy train:  0.9921875 Accuracy validation:  0.9538333333333333\n",
      "800 loss:  0.02081349542134791 Accuracy train:  1.0 Accuracy validation:  0.9551666666666667\n",
      "900 loss:  0.015419021902448345 Accuracy train:  1.0 Accuracy validation:  0.9558333333333333\n",
      "1000 loss:  0.011884412860079248 Accuracy train:  1.0 Accuracy validation:  0.956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9529"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[28*28, 128, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.1)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  12.464585041668698 Accuracy train:  0.0859375 Accuracy validation:  0.09966666666666667\n",
      "100 loss:  0.46036860956570014 Accuracy train:  0.8359375 Accuracy validation:  0.8016666666666666\n",
      "200 loss:  0.3249040537795956 Accuracy train:  0.8828125 Accuracy validation:  0.84\n",
      "300 loss:  0.27078004060540584 Accuracy train:  0.921875 Accuracy validation:  0.8585\n",
      "400 loss:  0.2416770151301409 Accuracy train:  0.9453125 Accuracy validation:  0.8726666666666667\n",
      "500 loss:  0.22627412442266304 Accuracy train:  0.953125 Accuracy validation:  0.88\n",
      "600 loss:  0.21628687959147463 Accuracy train:  0.9609375 Accuracy validation:  0.887\n",
      "700 loss:  0.2073365304456452 Accuracy train:  0.96875 Accuracy validation:  0.8923333333333333\n",
      "800 loss:  0.2004104647697995 Accuracy train:  0.96875 Accuracy validation:  0.897\n",
      "900 loss:  0.19321834614803846 Accuracy train:  0.96875 Accuracy validation:  0.9001666666666667\n",
      "1000 loss:  0.18745422490512803 Accuracy train:  0.96875 Accuracy validation:  0.904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9048"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[28*28, 128, 64, 10],activations = [\"relu\",\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44000, 3072) (44000, 10)\n",
      "(10000, 3072) (10000, 10)\n",
      "(6000, 3072) (6000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "from keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train /255.0\n",
    "x_test = x_test /255.0\n",
    "x_validation = x_train[:6000]\n",
    "y_validation = y_train[:6000]\n",
    "x_train = x_train[6000:]\n",
    "y_train = y_train[6000:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_validation = x_validation.reshape(x_validation.shape[0],-1)\n",
    "x_train = x_train.reshape(x_train.shape[0],-1)\n",
    "x_test = x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))\n",
    "  one_hot_Y[np.arange(Y.size),Y] = 1\n",
    "  return one_hot_Y\n",
    "\n",
    "y_train = one_hot(y_train.T)\n",
    "y_validation = one_hot(y_validation.T)\n",
    "y_test = one_hot(y_test.T)\n",
    "\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "print(x_validation.shape,y_validation.shape)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  12.196040603259688 Accuracy train:  0.1015625 Accuracy validation:  0.07466666666666667\n",
      "100 loss:  1.9087631345685416 Accuracy train:  0.28125 Accuracy validation:  0.2735\n",
      "200 loss:  1.8778539629644588 Accuracy train:  0.3046875 Accuracy validation:  0.2975\n",
      "300 loss:  1.8508386718222158 Accuracy train:  0.328125 Accuracy validation:  0.31016666666666665\n",
      "400 loss:  1.8327605065217207 Accuracy train:  0.3515625 Accuracy validation:  0.31933333333333336\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(layers_size=[32*32*3, 128, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset/test/corn/corn997.jpg has an unexpected shape: (32, 32)\n",
      "(7000, 3072) (7000, 10)\n",
      "(1000, 3072) (1000, 10)\n",
      "(1999, 3072) (1999, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "train_path = \"dataset/train.csv\"\n",
    "val_path = \"dataset/val.csv\"\n",
    "test_path = \"dataset/test.csv\"\n",
    "resize_to = (32, 32)\n",
    "x_train = []\n",
    "x_validation = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_validation = []\n",
    "y_test = []\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hình ảnh\n",
    "        data = np.asarray(image)\n",
    "        x_train.append(data)\n",
    "        y_train.append(slip_data[1])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "with open(val_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hình ảnh\n",
    "        data = np.asarray(image)\n",
    "        x_validation.append(data)\n",
    "        y_validation.append(slip_data[1])\n",
    "x_validation = np.array(x_validation)\n",
    "y_validation = np.array(y_validation)\n",
    "\n",
    "\n",
    "\n",
    "with open(test_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hình ảnh\n",
    "        data = np.asarray(image)\n",
    "        if data.shape == (32, 32, 3):\n",
    "            x_test.append(data)\n",
    "            y_test.append(slip_data[1])\n",
    "        else:\n",
    "            print(f\"Image {image_path} has an unexpected shape: {data.shape}\")\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],-1) /255.0\n",
    "x_validation = x_validation.reshape(x_validation.shape[0],-1) /255.0\n",
    "x_test = x_test.reshape(x_test.shape[0],-1) /255.0\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, np.max(Y) + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_train = one_hot(y_train)\n",
    "\n",
    "y_validation = y_validation.astype(int)\n",
    "y_validation = one_hot(y_validation)\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "y_test = one_hot(y_test)\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(x_validation.shape,y_validation.shape)\n",
    "\n",
    "print(x_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 loss train: 12.255424619951564 Accuracy train: 0.10957142857142857 loss val: 12.128805919633344 Accuracy validation:  0.119\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "nn = NeuralNetwork(layers_size=[32*32*3, 128, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_autograd import *\n",
    "nn = NeuralNetwork(layers_size=[32*32*3,256, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_autograd import *\n",
    "nn = NeuralNetwork(layers_size=[32*32*3,256, 10],activations = [\"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 64)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_autograd import *\n",
    "nn = NeuralNetwork(layers_size=[32*32*3,512,128, 10],activations = [\"relu\", \"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 128)\n",
    "nn.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_autograd import *\n",
    "nn = NeuralNetwork(layers_size=[32*32*3,512,128, 10],activations = [\"relu\", \"relu\", \"softmax\"],lossFunction = \"crossEntropy\", l_rate=0.01)\n",
    "nn.train(x_train, y_train,x_validation, y_validation, epochs=1001,batch_size = 64)\n",
    "nn.test(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
